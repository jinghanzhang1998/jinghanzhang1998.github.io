---
title: "Distilling Empathy from Large Language Models"
collection: publications
category: conferences
permalink: /publication/2025-sigdial-empathy-distillation
date: 2025-07-10
venue: "Proceedings of the 26th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2025)"
authors: "Henry J. Xie, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu"
paperurl: "https://arxiv.org/abs/2507.08151"
excerpt: "This paper presents a two-step fine-tuning framework for distilling empathy from Large Language Models (LLMs) into Smaller Language Models (SLMs), achieving a 90% win rate in empathetic response generation."
---

The distillation of knowledge from Large Language Models (LLMs) into Smaller Language Models (SLMs), preserving the capabilities and performance of LLMs while reducing model size, has played a key role in the proliferation of LLMs. Because SLMs are considerably smaller than LLMs, they are often utilized in domains where human interaction is frequent but resources are highly constrained, e.g., smart phones. Therefore, it is crucial to ensure that empathy, a fundamental aspect of positive human interactions, already instilled into LLMs, is retained by SLMs after distillation. In this paper, we develop a comprehensive approach for effective empathy distillation from LLMs into SLMs. Our approach features a two-step fine-tuning process that fully leverages datasets of empathetic dialogue responses distilled from LLMs. We explore several distillation methods beyond basic direct prompting and propose four unique sets of prompts for targeted empathy improvement to significantly enhance the empathy distillation process. Our evaluations demonstrate that SLMs fine-tuned through the two-step fine-tuning process with distillation datasets enhanced by the targeted empathy improvement prompts significantly outperform the base SLM at generating empathetic responses with a win rate of 90%. Our targeted empathy improvement prompts substantially outperform the basic direct prompting with a 10% improvement in win rate.

ðŸ“„ *In Proceedings of the 26th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2025).*  
ðŸ”— [Paper Link (arXiv:2507.08151)](https://arxiv.org/abs/2507.08151)
